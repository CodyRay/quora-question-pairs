% cs434-sp17 final paper, hoeftc and gillenp
% based on nips_2017.tex

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

% print author's names
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}
\usepackage{caption}

\title{Finding Quora questions pairs with neural networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
    Cody Ray Hoeft \\
    School of EECS \\
    Oregon State University \\
    Corvallis, OR 97331 \\
    \texttt{hoeftc@oregonstate.edu} \\
    \And
    Padraig Gillen \\
    School of EECS \\
    Oregon State University \\
    Corvallis, OR 97331 \\
    \texttt{gillenp@oregonstate.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
    The website Quora helps anybody ask and answer questions. With 100 million monthly users, questions with the same intent are frequently asked. Questions with the same intent can lead to more overhead for Quora, readers, and writers. Quora challenged the Kaggle Community to find a better way to identify question pairs. We started with a starter Kernal from the Kaggle Community and experimented with changing the word embedding and trying different neural networks.
\end{abstract}

\section{Introduction}
% An introduction section that briefly describes the problem you aimed to solve and a summary of the main results

\section{Approach}
% One / multiple sections describing the approach(es) you explored for solving the problem.
% Examples: feature design, preprocessing, and the prediction models etc.

\subsection{Preproccessing}

Various methods of preproccessing the questions were investigated. In order to be used with word embeddings, words have to be identified within the sentence. The simplest preproccessing is to split the sentence into words based off of white space. The weakness is that some punctuation can cause words to be different even though they are the same; for example "love?" and "love" should be interpreted as the same word. Striping punctuation. Replacing punctuation with spaces (except for apostrophe) makes words touching punctuation the same. The capitalization of words can also cause words to be different, so words are made lowercase. Stop words, or words that are so common that they carry no extra meaning. So stop words can be removed from the sentence. Additionally, some words have the same roots but different, i.e., 'remove' and 'removing'. Stemming can be used tor convert both words to the same word. Additionally, many words in the training and testing set are misspelled. Automatic spell checking could correct the misspelled words.

\subsection{Different Word Embeddings}

Text data by itself doesn't contain much meaningful data. Word embeddings map words to vectors that have more meaning than the text itself. Building embeddings requires extremely large corpuses because they are finding a mapping from sparse data to dense data. Because the provided training and testing data are too small to train embeddings, we did not attempt to train one from scratch. Instead we tried using pretrained models including a model trained on a Google News corpus and Glove embeddings trained on a variety of sources.

Attempts were also made to improve the embeddings themselves. The embeddings contain both capitalized words and lower case words. For example, "Trump" and "trump" both exist in the Google News embeddings, but 'Trump' will have occurred more in the corpus and therefore the vector should have more meaning. Since preprocessed sentences can never contain capital words, we can remap "Trump" to "trump" and discard the original "trump". Likewise words that cannot possibly exist in the preprocessed sentences can be discarded to improve load time of the embeddings.

\subsection{Different Neural Networks}

\begin{center}
  \includegraphics[width=\textwidth]{NeuralNetStructure}
  \captionof{figure}{Outline of the neural network structure used}
  \label{nns}
\end{center}

The structure used in Figure~\ref{nns} was used to support three different types of neural networks which replace the orange block in the neural network.

A Long Short Term Memory (LSTM) network is a typical way to perform sentiment analysis on text. An LSTM is a reasonable choice for this problem because recognizing similar intent in a question is similar to sentiment analysis.

A Gated Recurrent Unit (GRU) network is similar to an LSTM with similar performance but has fewer internal parameters to train.

A Convolutional Neural Network (CNN) is a network made of filters that is commonly used in image processing. A CNN is a reasonable choice for this problem because it has the potential to extract similarities in the sentences in different parts of the sentences.

\begin{center}
  \includegraphics[width=\textwidth]{LetterNeuralNetStructure}
  \captionof{figure}{Outline of the neural network structure used for letter-wise neural networks}
  \label{lnns}
\end{center}

In addition to the original structure, which used word embeddings. CNN and GRU neural nets were also attempted over letters instead of words. As shown in Figure~\ref{lnns}, the location in the neural network where the two paths were concatenated was also moved before the CNN layer. To enable the encoding of the letters a 2 dimension word2vec instance was trained from scratch using each letter as a word. After the embedding the each letter, the 2d vectors for each letter are joined into a 4d vector pairwise for the first 150 letters.

\section{Results}
% section devoted to presenting / discussing the results  obtained from exploration
% possible discussion topics include:
%   what worked and what didn't?
%   How do different methods compare with each other?
%   what kind of lessons you learned from your exploration?
%   What are possible ways for future work to improve?

\subsection{Preproccessing}

Stripping punctuation and making the sentence lowercase improved the validation loss. However, removing stop words and stemming were found to make the validation loss worse. Automatic spell check with a method explained on Kaggle, was attempted however it was found to be too resource intensive.

\subsection{Word Embeddings}

There was little difference between the validation loss of models trained on the Google News corpus and the Glove embeddings. In general, the Google News vectors performed better however the test loss difference was smaller than the test loss difference between the same model being run with different validation splits.

Efforts to improve the embeddings themselves yielded very small improvements in test loss. For Google News, the improvement was $0.00548$ and for Glove the improvement was $0.00095$. However, the size of the embeddings were reduced by $0.47$ GB and $1.22$ GB respectively. This helps them load faster.

\subsection{Different Neural Networks}

We started with an LSTM network which achieved a score of $0.30802$ using Glove embeddings. The GRU network achieved a slightly better score of $0.30411$. The CNN network was less successful, with a score of $0.33798$.

The letter-wise GRU network failed to get less than a loss of $0.4$ after several epochs of training. However, the letter-wise CNN worked surprisingly well and yielded a score of $0.36233$.

\section{Conclusion}
% A conclusion section that summarize your project and your effort.

\appendix
\section{Work Done}
% appendix describes individual contribution levels to the project
%    estimated in percentages and described in words

\end{document}

% \subsection{Citations within the text}
%
% The \verb+natbib+ package will be loaded for you by default.
% Citations may be author/year or numeric, as long as you maintain
% internal consistency.  As to the format of the references themselves,
% any style is acceptable as long as it is used consistently.
%
% The documentation for \verb+natbib+ may be found at
% \begin{center}
%     \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
%     \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%     Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}
%
% If you wish to load the \verb+natbib+ package with options, you may
% add the following before loading the \verb+nips_2017+ package:
% \begin{verbatim}
%     \PassOptionsToPackage{options}{natbib}
% \end{verbatim}
%
% If \verb+natbib+ clashes with another package you load, you can add
% the optional argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%     \usepackage[nonatbib]{nips_2017}
% \end{verbatim}
%
% As submission is double blind, refer to your own published work in the
% third person. That is, use ``In the previous work of Jones et
% al.\ [4],'' not ``In our previous work [4].'' If you cite your other
% papers that are not widely available (e.g., a journal paper under
% review), use anonymous author names in the citation, e.g., an author
% of the form ``A.\ Anonymous.''
%
%
% \subsection{Tables}
%
% All tables must be centered, neat, clean and legible.  The table
% number and title always appear before the table.  See
% Table~\ref{sample-table}.
%
% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.
%
% Note that publication-quality tables \emph{do not contain vertical
%   rules.} We strongly suggest the use of the \verb+booktabs+ package,
% which allows for typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.
%
% \begin{table}[t]
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule{1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}
